{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - Code Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allison Collins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file, I will run my updated pipeline on the kaggle credit dataset. Given the number of thresholds we were asked to compare, I reduced the different values of each of the other parameters from my initial exploration, as I did not see huge differences in performance, to improve the readability and length of this document.\n",
    "\n",
    "I run each model individually with the associated performance outputs to try to make it clearer given the volume of information. Additionally, the baseline is computed using the sklearn dummy classifier as a baseline metric.\n",
    "\n",
    "***A few notes and citations: \n",
    "\n",
    "-- I consulted lab materials both here and in the pipeline document (specifically for adjusting thresholds & calculations from predicted scores; looping over models modifying parameters; investigating feature importance.) \n",
    "\n",
    "--At the end of each section per type of model, I include a chart which has all of the performance metrics across models. The date, threshold, and other (specific to the model) parameter values are included in the leftmost column. \n",
    "\n",
    "--I will call out in the different sections some models with better performance; the policy memo will stick to the implications for implementations and avoid too much technical language as per the instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import the pipeline file and a few other modules we might need in cleaning the data (specific to this dataset and the task at hand)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pipeline_revised\n",
    "import pandas as pd\n",
    "import pydot\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the data\n",
    "df = pipeline_revised.read_file('projects_2012_2013.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check its contents\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this particular dataset, our target variable is time to funding, so we will need to create an additional column which looks at the time required to get full funding (e.g. the difference between date posted and date fully funded, since this is not explicitly included in the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>projectid</th>\n",
       "      <th>teacher_acctid</th>\n",
       "      <th>schoolid</th>\n",
       "      <th>school_ncesid</th>\n",
       "      <th>school_latitude</th>\n",
       "      <th>school_longitude</th>\n",
       "      <th>school_city</th>\n",
       "      <th>school_state</th>\n",
       "      <th>school_metro</th>\n",
       "      <th>school_district</th>\n",
       "      <th>...</th>\n",
       "      <th>secondary_focus_area</th>\n",
       "      <th>resource_type</th>\n",
       "      <th>poverty_level</th>\n",
       "      <th>grade_level</th>\n",
       "      <th>total_price_including_optional_support</th>\n",
       "      <th>students_reached</th>\n",
       "      <th>eligible_double_your_impact_match</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>datefullyfunded</th>\n",
       "      <th>time_to_funding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001ccc0e81598c4bd86bacb94d7acb</td>\n",
       "      <td>96963218e74e10c3764a5cfb153e6fea</td>\n",
       "      <td>9f3f9f2c2da7edda5648ccd10554ed8c</td>\n",
       "      <td>1.709930e+11</td>\n",
       "      <td>41.807654</td>\n",
       "      <td>-87.673257</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>urban</td>\n",
       "      <td>Pershing Elem Network</td>\n",
       "      <td>...</td>\n",
       "      <td>Music &amp; The Arts</td>\n",
       "      <td>Supplies</td>\n",
       "      <td>highest poverty</td>\n",
       "      <td>Grades PreK-2</td>\n",
       "      <td>1498.61</td>\n",
       "      <td>31.0</td>\n",
       "      <td>f</td>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>2013-05-02</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          projectid                    teacher_acctid  \\\n",
       "0  00001ccc0e81598c4bd86bacb94d7acb  96963218e74e10c3764a5cfb153e6fea   \n",
       "\n",
       "                           schoolid  school_ncesid  school_latitude  \\\n",
       "0  9f3f9f2c2da7edda5648ccd10554ed8c   1.709930e+11        41.807654   \n",
       "\n",
       "   school_longitude school_city school_state school_metro  \\\n",
       "0        -87.673257     Chicago           IL        urban   \n",
       "\n",
       "         school_district  ... secondary_focus_area resource_type  \\\n",
       "0  Pershing Elem Network  ...     Music & The Arts      Supplies   \n",
       "\n",
       "     poverty_level    grade_level total_price_including_optional_support  \\\n",
       "0  highest poverty  Grades PreK-2                                1498.61   \n",
       "\n",
       "  students_reached eligible_double_your_impact_match date_posted  \\\n",
       "0             31.0                                 f  2013-04-14   \n",
       "\n",
       "  datefullyfunded time_to_funding  \n",
       "0      2013-05-02              18  \n",
       "\n",
       "[1 rows x 27 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['datefullyfunded'] = pd.to_datetime(df.datefullyfunded)\n",
    "df['date_posted'] = pd.to_datetime(df.date_posted)\n",
    "df['time_to_funding'] = (df.datefullyfunded - df.date_posted).dt.days\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we will have to create a binary outcome \"label\" column which assigns 1 or 0 based on whether the time to funding was over or under 60. We will also convert a few of the categorical variables to dummy columns so that we can include those in our analysis as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will convert the time-to-funding column to be binary as required for some of the models\n",
    "df['time_tf'] = np.where(df['time_to_funding']>=60, 1, 0)\n",
    "\n",
    "#We will convert a few columns to use in our analysis\n",
    "#df = pipeline_revised.create_binary_col(df, 'eligible_double_your_impact_match', {'t':1,'f':0})\n",
    "#df = pipeline_revised.discretize_categorical(df, ['poverty_level','resource_type','primary_focus_subject','school_metro'])\n",
    "\n",
    "#Now we can look at columns to grab which we want as predictors\n",
    "#df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = pipeline_revised.temporal_split(\n",
    "    df,'date_posted','2013-05-31',180,60,'time_tf',predictors=[\n",
    "        'total_price_including_optional_support','students_reached']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[1498.61  282.47 1012.38 ...  184.05  345.84  938.  ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-cb35e7703fc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipeline_revised\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodels_to_run\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/GitHub/ML-Homework-5/pipeline_revised.py\u001b[0m in \u001b[0;36mrun_models\u001b[0;34m(x_train, x_test, y_train, y_test, models_to_run)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameter_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m                 \u001b[0mpred_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 results_df.loc[len(results_df)] = [models_to_run[index],clf, p, current_outcome, validation_date, group,\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    799\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    802\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    550\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[1498.61  282.47 1012.38 ...  184.05  345.84  938.  ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "pipeline_revised.run_models(x_train, x_test, y_train, y_test,models_to_run='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this particular dataset, we have a number of categorical columns; in this case, I will opt to drop NAs; with the information we have on hand, it doesn't make sense to impute categories such as a school's focus area or poverty level. I'm subsetting this to columns planned to include in the analysis -- so if there is a na in a different column, we will retain it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Current shape:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis=0,subset=['time_to_funding','total_price_including_optional_support','students_reached'])\n",
    "print('New shape:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now look at some summary stats for the numerical columns with the cleaned dataset we are going to be working with. We can see that the average time to funding is 48 days, with a median of 41, so it is skewed upwards. The average total price is about 670 dollars, and on averge 90 students are reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_revised.calc_summary_stats(df,cols_to_include=['time_to_funding','total_price_including_optional_support','students_reached'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will convert the time-to-funding column to be binary as required for some of the models\n",
    "df['time_tf'] = np.where(df['time_to_funding']>=60, 1, 0)\n",
    "\n",
    "#We will convert a few columns to use in our analysis\n",
    "df = pipeline_revised.create_binary_col(df, 'eligible_double_your_impact_match', {'t':1,'f':0})\n",
    "df = pipeline_revised.discretize_categorical(df, ['poverty_level','resource_type','primary_focus_subject','school_metro'])\n",
    "\n",
    "#Now we can look at columns to grab which we want as predictors\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is in the right format, we will work through the different models and look at their evaluation performance.\n",
    "\n",
    "We can first start with the decision tree. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_list = ['2013-06-30', '2013-05-31', '2013-04-30']\n",
    "end_list = ['2013-12-31', '2013-11-30','2013-10-31']\n",
    "thresholds = [.01,.02,.05,.1,.2,.3,.5]\n",
    "depth_list = [3, 4]\n",
    "trees = []\n",
    "\n",
    "for i, date in enumerate(start_list):\n",
    "    end_date = end_list[i]\n",
    "    for depth in depth_list:\n",
    "        for threshold in thresholds:\n",
    "            tree_name = 'tree_' + date + '_' + str(threshold) + '_' + str(depth)\n",
    "            y_test, y_predict, y_score, features = pipeline_revised.create_decision_tree(df, 'time_tf', depth ,500 ,tree_name, threshold=threshold,\n",
    "                                                          predictors=['total_price_including_optional_support', 'students_reached','poverty_level_high poverty', 'poverty_level_highest poverty',\n",
    "       'poverty_level_low poverty', 'poverty_level_moderate poverty','resource_type_Books', 'resource_type_Other', 'resource_type_Supplies',\n",
    "       'resource_type_Technology', 'resource_type_Trips',\n",
    "       'resource_type_Visitors','school_metro_rural',\n",
    "       'school_metro_suburban', 'school_metro_urban','primary_focus_subject_Applied Sciences',\n",
    "       'primary_focus_subject_Character Education',\n",
    "       'primary_focus_subject_Civics & Government',\n",
    "       'primary_focus_subject_College & Career Prep',\n",
    "       'primary_focus_subject_Community Service', 'primary_focus_subject_ESL',\n",
    "       'primary_focus_subject_Early Development',\n",
    "       'primary_focus_subject_Economics',\n",
    "       'primary_focus_subject_Environmental Science',\n",
    "       'primary_focus_subject_Extracurricular',\n",
    "       'primary_focus_subject_Foreign Languages',\n",
    "       'primary_focus_subject_Gym & Fitness',\n",
    "       'primary_focus_subject_Health & Life Science',\n",
    "       'primary_focus_subject_Health & Wellness',\n",
    "       'primary_focus_subject_History & Geography',\n",
    "       'primary_focus_subject_Literacy',\n",
    "       'primary_focus_subject_Literature & Writing',\n",
    "       'primary_focus_subject_Mathematics', 'primary_focus_subject_Music',\n",
    "       'primary_focus_subject_Nutrition', 'primary_focus_subject_Other',\n",
    "       'primary_focus_subject_Parent Involvement',\n",
    "       'primary_focus_subject_Performing Arts',\n",
    "       'primary_focus_subject_Social Sciences',\n",
    "       'primary_focus_subject_Special Needs', 'primary_focus_subject_Sports',\n",
    "       'primary_focus_subject_Visual Arts'],temporal=True, start_col='date_posted',end_col='datefullyfunded',start_date=date,end_date=end_date)\n",
    "            \n",
    "            trees.append((tree_name,y_test,y_predict, y_score, features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a table which shows the performance of the different iterations of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_metrics = pd.DataFrame(columns=['name','accuracy','precision','auc','recall','f1'])\n",
    "\n",
    "for i in range(len(trees)):\n",
    "    name, test, predict, _, _ = trees[i]\n",
    "    accuracy, precision, auc, recall, f1 = pipeline_revised.evaluate_model(test, predict)\n",
    "    tree_metrics.loc[i,'name'] = name\n",
    "    tree_metrics.loc[i,'accuracy'] = accuracy\n",
    "    tree_metrics.loc[i,'precision'] = precision\n",
    "    tree_metrics.loc[i,'auc'] = auc\n",
    "    tree_metrics.loc[i,'recall'] = recall\n",
    "    tree_metrics.loc[i,'f1'] = f1\n",
    "\n",
    "tree_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of trees (as will happen with later models) have precision / recall of 0 because they were lacking true positives. Let's visualize the one which has the highest area under the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(graph,) = pydot.graph_from_dot_file('tree_2013-06-30_0.3_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.write_png('tree_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"tree_1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Citation for precision recall curve - sklearn documentation\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils.fixes import signature\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "_, test, _, score, _ = trees[12]\n",
    "clean_score = [x[1] for x in score]\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(test, clean_score)\n",
    "average_precision = average_precision_score(test, clean_score)\n",
    "\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be discussed in the policy write-up, but overall the performance across the models will be fairly similar. Investigating model documentation suggests that feature selection can be the most important -- selecting on different models with good or bad features will perform in line with that. Let's visualize the most important features for this classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores = (trees[12][4])\n",
    "feature_names = ['total_price_including_optional_support', 'students_reached','poverty_level_high poverty', 'poverty_level_highest poverty',\n",
    "       'poverty_level_low poverty', 'poverty_level_moderate poverty','resource_type_Books', 'resource_type_Other', 'resource_type_Supplies',\n",
    "       'resource_type_Technology', 'resource_type_Trips',\n",
    "       'resource_type_Visitors','school_metro_rural',\n",
    "       'school_metro_suburban', 'school_metro_urban','primary_focus_subject_Applied Sciences',\n",
    "       'primary_focus_subject_Character Education',\n",
    "       'primary_focus_subject_Civics & Government',\n",
    "       'primary_focus_subject_College & Career Prep',\n",
    "       'primary_focus_subject_Community Service', 'primary_focus_subject_ESL',\n",
    "       'primary_focus_subject_Early Development',\n",
    "       'primary_focus_subject_Economics',\n",
    "       'primary_focus_subject_Environmental Science',\n",
    "       'primary_focus_subject_Extracurricular',\n",
    "       'primary_focus_subject_Foreign Languages',\n",
    "       'primary_focus_subject_Gym & Fitness',\n",
    "       'primary_focus_subject_Health & Life Science',\n",
    "       'primary_focus_subject_Health & Wellness',\n",
    "       'primary_focus_subject_History & Geography',\n",
    "       'primary_focus_subject_Literacy',\n",
    "       'primary_focus_subject_Literature & Writing',\n",
    "       'primary_focus_subject_Mathematics', 'primary_focus_subject_Music',\n",
    "       'primary_focus_subject_Nutrition', 'primary_focus_subject_Other',\n",
    "       'primary_focus_subject_Parent Involvement',\n",
    "       'primary_focus_subject_Performing Arts',\n",
    "       'primary_focus_subject_Social Sciences',\n",
    "       'primary_focus_subject_Special Needs', 'primary_focus_subject_Sports',\n",
    "       'primary_focus_subject_Visual Arts']\n",
    "\n",
    "d = {'Features': feature_names, \"Importance\": feature_scores}\n",
    "feature_importance = pd.DataFrame(data=d)\n",
    "feature_importance = feature_importance.sort_values(by=['Importance'], ascending=False)\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the most relevant features are the total price of the request; the school being in an urban area; and it being directed toward trips."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbor Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_list = ['2013-06-30', '2013-05-31', '2013-04-30']\n",
    "end_list = ['2013-12-31', '2013-11-30','2013-10-31']\n",
    "neighbor_list = [10, 50]\n",
    "thresholds = [.01,.02,.05,.1,.2,.3,.5]\n",
    "metrics = [\"euclidean\", \"minkowski\"]\n",
    "neighbors = []\n",
    "\n",
    "for i, date in enumerate(start_list):\n",
    "    end_date = end_list[i]\n",
    "    for n in neighbor_list:\n",
    "        for metric in metrics:\n",
    "            for threshold in thresholds:\n",
    "                model_name = 'knn_' + date + '_' + str(n) + '_' + str(threshold) +'_' + metric\n",
    "                p_test, p_predict, p_score, params = pipeline_revised.knn_classify(df, 'time_tf', n, metric, threshold = threshold,\n",
    "                                                                               predictors=['total_price_including_optional_support', 'students_reached','poverty_level_high poverty', 'poverty_level_highest poverty',\n",
    "       'poverty_level_low poverty', 'poverty_level_moderate poverty','resource_type_Books', 'resource_type_Other', 'resource_type_Supplies',\n",
    "       'resource_type_Technology', 'resource_type_Trips',\n",
    "       'resource_type_Visitors','school_metro_rural',\n",
    "       'school_metro_suburban', 'school_metro_urban','primary_focus_subject_Applied Sciences',\n",
    "       'primary_focus_subject_Character Education',\n",
    "       'primary_focus_subject_Civics & Government',\n",
    "       'primary_focus_subject_College & Career Prep',\n",
    "       'primary_focus_subject_Community Service', 'primary_focus_subject_ESL',\n",
    "       'primary_focus_subject_Early Development',\n",
    "       'primary_focus_subject_Economics',\n",
    "       'primary_focus_subject_Environmental Science',\n",
    "       'primary_focus_subject_Extracurricular',\n",
    "       'primary_focus_subject_Foreign Languages',\n",
    "       'primary_focus_subject_Gym & Fitness',\n",
    "       'primary_focus_subject_Health & Life Science',\n",
    "       'primary_focus_subject_Health & Wellness',\n",
    "       'primary_focus_subject_History & Geography',\n",
    "       'primary_focus_subject_Literacy',\n",
    "       'primary_focus_subject_Literature & Writing',\n",
    "       'primary_focus_subject_Mathematics', 'primary_focus_subject_Music',\n",
    "       'primary_focus_subject_Nutrition', 'primary_focus_subject_Other',\n",
    "       'primary_focus_subject_Parent Involvement',\n",
    "       'primary_focus_subject_Performing Arts',\n",
    "       'primary_focus_subject_Social Sciences',\n",
    "       'primary_focus_subject_Special Needs', 'primary_focus_subject_Sports',\n",
    "       'primary_focus_subject_Visual Arts'],temporal=True, start_col='date_posted',end_col='datefullyfunded',start_date=date,end_date=end_date)\n",
    "                neighbors.append((model_name, p_test, p_predict, p_score, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a table which shows the performance of the different iterations of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_metrics = pd.DataFrame(columns=['name','accuracy','precision','auc','recall','f1'])\n",
    "\n",
    "for i in range(len(neighbors)):\n",
    "    name, test, predict, _, _ = neighbors[i]\n",
    "    accuracy, precision, auc, recall, f1 = pipeline_revised.evaluate_model(test, predict)\n",
    "    knn_metrics.loc[i,'name'] = name\n",
    "    knn_metrics.loc[i,'accuracy'] = accuracy\n",
    "    knn_metrics.loc[i,'precision'] = precision\n",
    "    knn_metrics.loc[i,'auc'] = auc\n",
    "    knn_metrics.loc[i,'recall'] = recall\n",
    "    knn_metrics.loc[i,'f1'] = f1\n",
    "\n",
    "knn_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top performing ones from an area under the curve perspective had number of neighbors of 50, threshold .3 -- but the distance mechanism didn't matter. We can check the characteristics for one of these if we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neighbors[80][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_list = ['2013-06-30', '2013-05-31', '2013-04-30']\n",
    "end_list = ['2013-12-31', '2013-11-30','2013-10-31']\n",
    "thresholds = [.01,.02,.05,.1,.2,.3,.5]\n",
    "penalties = ['l1','l2']\n",
    "c_values = [.01, 1]\n",
    "regressions = []\n",
    "\n",
    "for i, date in enumerate(start_list):\n",
    "    end_date = end_list[i]\n",
    "    for t in thresholds:\n",
    "        for penalty in penalties:\n",
    "            for c in c_values:\n",
    "                model_name = 'reg_' + date + '_' + str(t) + '_' + penalty + '_' + str(c)\n",
    "                r_test, r_predict, r_score, coeffs = pipeline_revised.logistic_classify(df, 'time_tf', penalty, c, threshold = t,\n",
    "                                                                               predictors=['total_price_including_optional_support', 'students_reached','poverty_level_high poverty', 'poverty_level_highest poverty',\n",
    "       'poverty_level_low poverty', 'poverty_level_moderate poverty','resource_type_Books', 'resource_type_Other', 'resource_type_Supplies',\n",
    "       'resource_type_Technology', 'resource_type_Trips',\n",
    "       'resource_type_Visitors','school_metro_rural',\n",
    "       'school_metro_suburban', 'school_metro_urban','primary_focus_subject_Applied Sciences',\n",
    "       'primary_focus_subject_Character Education',\n",
    "       'primary_focus_subject_Civics & Government',\n",
    "       'primary_focus_subject_College & Career Prep',\n",
    "       'primary_focus_subject_Community Service', 'primary_focus_subject_ESL',\n",
    "       'primary_focus_subject_Early Development',\n",
    "       'primary_focus_subject_Economics',\n",
    "       'primary_focus_subject_Environmental Science',\n",
    "       'primary_focus_subject_Extracurricular',\n",
    "       'primary_focus_subject_Foreign Languages',\n",
    "       'primary_focus_subject_Gym & Fitness',\n",
    "       'primary_focus_subject_Health & Life Science',\n",
    "       'primary_focus_subject_Health & Wellness',\n",
    "       'primary_focus_subject_History & Geography',\n",
    "       'primary_focus_subject_Literacy',\n",
    "       'primary_focus_subject_Literature & Writing',\n",
    "       'primary_focus_subject_Mathematics', 'primary_focus_subject_Music',\n",
    "       'primary_focus_subject_Nutrition', 'primary_focus_subject_Other',\n",
    "       'primary_focus_subject_Parent Involvement',\n",
    "       'primary_focus_subject_Performing Arts',\n",
    "       'primary_focus_subject_Social Sciences',\n",
    "       'primary_focus_subject_Special Needs', 'primary_focus_subject_Sports',\n",
    "       'primary_focus_subject_Visual Arts'],temporal=True, start_col='date_posted',end_col='datefullyfunded',start_date=date,end_date=end_date)\n",
    "                regressions.append((model_name, r_test, r_predict, r_score, coeffs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a table which shows the performance of the different iterations of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 100)\n",
    "log_metrics = pd.DataFrame(columns=['name','accuracy','precision','auc','recall','f1'])\n",
    "\n",
    "for i in range(len(regressions)):\n",
    "    name, test, predict, _, _ = regressions[i]\n",
    "    accuracy, precision, auc, recall, f1 = pipeline_revised.evaluate_model(test, predict)\n",
    "    log_metrics.loc[i,'name'] = name\n",
    "    log_metrics.loc[i,'accuracy'] = accuracy\n",
    "    log_metrics.loc[i,'precision'] = precision\n",
    "    log_metrics.loc[i,'auc'] = auc\n",
    "    log_metrics.loc[i,'recall'] = recall\n",
    "    log_metrics.loc[i,'f1'] = f1\n",
    "\n",
    "log_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression 21 had the highest area under the curve, and one of the higher accuracies; we can visualize the weights given to the different features if we wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regressions[21][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_list = ['2013-06-30', '2013-05-31', '2013-04-30']\n",
    "end_list = ['2013-12-31', '2013-11-30','2013-10-31']\n",
    "thresholds = [.01,.02,.05,.1,.2,.3,.5]\n",
    "c_values = [1, 10]\n",
    "svm = []\n",
    "\n",
    "for i, date in enumerate(start_list):\n",
    "    end_date = end_list[i]\n",
    "    for t in thresholds:\n",
    "        for c in c_values:\n",
    "            model_name = 'svm_' + date + '_' + str(t) + '_' + str(c)\n",
    "            s_test, s_predict, s_score = pipeline_revised.SVM_classify(df, 'time_tf', c, threshold = t,\n",
    "                                                                               predictors=['total_price_including_optional_support', 'students_reached','poverty_level_high poverty', 'poverty_level_highest poverty',\n",
    "       'poverty_level_low poverty', 'poverty_level_moderate poverty','resource_type_Books', 'resource_type_Other', 'resource_type_Supplies',\n",
    "       'resource_type_Technology', 'resource_type_Trips',\n",
    "       'resource_type_Visitors','school_metro_rural',\n",
    "       'school_metro_suburban', 'school_metro_urban','primary_focus_subject_Applied Sciences',\n",
    "       'primary_focus_subject_Character Education',\n",
    "       'primary_focus_subject_Civics & Government',\n",
    "       'primary_focus_subject_College & Career Prep',\n",
    "       'primary_focus_subject_Community Service', 'primary_focus_subject_ESL',\n",
    "       'primary_focus_subject_Early Development',\n",
    "       'primary_focus_subject_Economics',\n",
    "       'primary_focus_subject_Environmental Science',\n",
    "       'primary_focus_subject_Extracurricular',\n",
    "       'primary_focus_subject_Foreign Languages',\n",
    "       'primary_focus_subject_Gym & Fitness',\n",
    "       'primary_focus_subject_Health & Life Science',\n",
    "       'primary_focus_subject_Health & Wellness',\n",
    "       'primary_focus_subject_History & Geography',\n",
    "       'primary_focus_subject_Literacy',\n",
    "       'primary_focus_subject_Literature & Writing',\n",
    "       'primary_focus_subject_Mathematics', 'primary_focus_subject_Music',\n",
    "       'primary_focus_subject_Nutrition', 'primary_focus_subject_Other',\n",
    "       'primary_focus_subject_Parent Involvement',\n",
    "       'primary_focus_subject_Performing Arts',\n",
    "       'primary_focus_subject_Social Sciences',\n",
    "       'primary_focus_subject_Special Needs', 'primary_focus_subject_Sports',\n",
    "       'primary_focus_subject_Visual Arts'],temporal=True, start_col='date_posted',end_col='datefullyfunded',start_date=date,end_date=end_date)\n",
    "            svm.append((model_name, s_test, s_predict, s_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create a table showing the performance of the different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_metrics = pd.DataFrame(columns=['name','accuracy','precision','auc','recall','f1'])\n",
    "\n",
    "for i in range(len(svm)):\n",
    "    name, test, predict, _, = svm[i]\n",
    "    accuracy, precision, auc, recall, f1 = pipeline_revised.evaluate_model(test, predict)\n",
    "    svm_metrics.loc[i,'name'] = name\n",
    "    svm_metrics.loc[i,'accuracy'] = accuracy\n",
    "    svm_metrics.loc[i,'precision'] = precision\n",
    "    svm_metrics.loc[i,'auc'] = auc\n",
    "    svm_metrics.loc[i,'recall'] = recall\n",
    "    svm_metrics.loc[i,'f1'] = f1\n",
    "\n",
    "svm_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, model # 10, with threshold of .3, has the highest area under the curve, but mediocre accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "Note - random forest models incorporate bagging; thus this serves for both the random forest and bagging model requirements outline in the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_list = ['2013-06-30', '2013-05-31', '2013-04-30']\n",
    "end_list = ['2013-12-31', '2013-11-30','2013-10-31']\n",
    "thresholds = [.01,.02,.05,.1,.2,.3,.5]\n",
    "max_depth = [3, 4]\n",
    "forest = []\n",
    "\n",
    "for i, date in enumerate(start_list):\n",
    "    end_date = end_list[i]\n",
    "    for t in thresholds:\n",
    "        for d in max_depth:\n",
    "            model_name = 'rf_' + date + '_' + str(t) + '_' + str(d)\n",
    "            f_test, f_predict, f_score, features = pipeline_revised.random_forest(df, 'time_tf', d, threshold = t,\n",
    "                                                                               predictors=['total_price_including_optional_support', 'students_reached','poverty_level_high poverty', 'poverty_level_highest poverty',\n",
    "       'poverty_level_low poverty', 'poverty_level_moderate poverty','resource_type_Books', 'resource_type_Other', 'resource_type_Supplies',\n",
    "       'resource_type_Technology', 'resource_type_Trips',\n",
    "       'resource_type_Visitors','school_metro_rural',\n",
    "       'school_metro_suburban', 'school_metro_urban','primary_focus_subject_Applied Sciences',\n",
    "       'primary_focus_subject_Character Education',\n",
    "       'primary_focus_subject_Civics & Government',\n",
    "       'primary_focus_subject_College & Career Prep',\n",
    "       'primary_focus_subject_Community Service', 'primary_focus_subject_ESL',\n",
    "       'primary_focus_subject_Early Development',\n",
    "       'primary_focus_subject_Economics',\n",
    "       'primary_focus_subject_Environmental Science',\n",
    "       'primary_focus_subject_Extracurricular',\n",
    "       'primary_focus_subject_Foreign Languages',\n",
    "       'primary_focus_subject_Gym & Fitness',\n",
    "       'primary_focus_subject_Health & Life Science',\n",
    "       'primary_focus_subject_Health & Wellness',\n",
    "       'primary_focus_subject_History & Geography',\n",
    "       'primary_focus_subject_Literacy',\n",
    "       'primary_focus_subject_Literature & Writing',\n",
    "       'primary_focus_subject_Mathematics', 'primary_focus_subject_Music',\n",
    "       'primary_focus_subject_Nutrition', 'primary_focus_subject_Other',\n",
    "       'primary_focus_subject_Parent Involvement',\n",
    "       'primary_focus_subject_Performing Arts',\n",
    "       'primary_focus_subject_Social Sciences',\n",
    "       'primary_focus_subject_Special Needs', 'primary_focus_subject_Sports',\n",
    "       'primary_focus_subject_Visual Arts'],temporal=True, start_col='date_posted',end_col='datefullyfunded',start_date=date,end_date=end_date)\n",
    "            forest.append((model_name, f_test, f_predict, f_score, features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now produce a chart of the models' relative performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_metrics = pd.DataFrame(columns=['name','accuracy','precision','auc','recall','f1'])\n",
    "\n",
    "for i in range(len(forest)):\n",
    "    name, test, predict, _,_ = forest[i]\n",
    "    accuracy, precision, auc, recall, f1 = pipeline_revised.evaluate_model(test, predict)\n",
    "    rf_metrics.loc[i,'name'] = name\n",
    "    rf_metrics.loc[i,'accuracy'] = accuracy\n",
    "    rf_metrics.loc[i,'precision'] = precision\n",
    "    rf_metrics.loc[i,'auc'] = auc\n",
    "    rf_metrics.loc[i,'recall'] = recall\n",
    "    rf_metrics.loc[i,'f1'] = f1\n",
    "\n",
    "rf_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that model 7 has the highest area under the curve, moderate accuract, and an average f1 (combined precision/recall) score among models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we wanted to see the feature values (associated with predictors input)\n",
    "#forest[7][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_list = ['2013-06-30', '2013-05-31', '2013-04-30']\n",
    "end_list = ['2013-12-31', '2013-11-30','2013-10-31']\n",
    "thresholds = [.01,.02,.05,.1,.2,.3,.5]\n",
    "max_features = [3, 4]\n",
    "gradient = []\n",
    "\n",
    "for i, date in enumerate(start_list):\n",
    "    end_date = end_list[i]\n",
    "    for t in thresholds:\n",
    "        for f in max_features:\n",
    "            model_name = 'gradient_' + date + '_' + str(t) + '_' + str(f)\n",
    "            g_test, g_predict, g_score = pipeline_revised.gradient_boost(df, 'time_tf', f, threshold = t,\n",
    "                                                                               predictors=['total_price_including_optional_support', 'students_reached','poverty_level_high poverty', 'poverty_level_highest poverty',\n",
    "       'poverty_level_low poverty', 'poverty_level_moderate poverty','resource_type_Books', 'resource_type_Other', 'resource_type_Supplies',\n",
    "       'resource_type_Technology', 'resource_type_Trips',\n",
    "       'resource_type_Visitors','school_metro_rural',\n",
    "       'school_metro_suburban', 'school_metro_urban','primary_focus_subject_Applied Sciences',\n",
    "       'primary_focus_subject_Character Education',\n",
    "       'primary_focus_subject_Civics & Government',\n",
    "       'primary_focus_subject_College & Career Prep',\n",
    "       'primary_focus_subject_Community Service', 'primary_focus_subject_ESL',\n",
    "       'primary_focus_subject_Early Development',\n",
    "       'primary_focus_subject_Economics',\n",
    "       'primary_focus_subject_Environmental Science',\n",
    "       'primary_focus_subject_Extracurricular',\n",
    "       'primary_focus_subject_Foreign Languages',\n",
    "       'primary_focus_subject_Gym & Fitness',\n",
    "       'primary_focus_subject_Health & Life Science',\n",
    "       'primary_focus_subject_Health & Wellness',\n",
    "       'primary_focus_subject_History & Geography',\n",
    "       'primary_focus_subject_Literacy',\n",
    "       'primary_focus_subject_Literature & Writing',\n",
    "       'primary_focus_subject_Mathematics', 'primary_focus_subject_Music',\n",
    "       'primary_focus_subject_Nutrition', 'primary_focus_subject_Other',\n",
    "       'primary_focus_subject_Parent Involvement',\n",
    "       'primary_focus_subject_Performing Arts',\n",
    "       'primary_focus_subject_Social Sciences',\n",
    "       'primary_focus_subject_Special Needs', 'primary_focus_subject_Sports',\n",
    "       'primary_focus_subject_Visual Arts'],temporal=True, start_col='date_posted',end_col='datefullyfunded',start_date=date,end_date=end_date)\n",
    "            gradient.append((model_name, g_test, g_predict, g_score))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now produce a chart of the models' relative performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_metrics = pd.DataFrame(columns=['name','accuracy','precision','auc','recall','f1'])\n",
    "\n",
    "for i in range(len(gradient)):\n",
    "    name, test, predict, _ = gradient[i]\n",
    "    accuracy, precision, auc, recall, f1 = pipeline_revised.evaluate_model(test, predict)\n",
    "    gradient_metrics.loc[i,'name'] = name\n",
    "    gradient_metrics.loc[i,'accuracy'] = accuracy\n",
    "    gradient_metrics.loc[i,'precision'] = precision\n",
    "    gradient_metrics.loc[i,'auc'] = auc\n",
    "    gradient_metrics.loc[i,'recall'] = recall\n",
    "    gradient_metrics.loc[i,'f1'] = f1\n",
    "\n",
    "gradient_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that model 11 has highest area under the curve and one of the higher accuracies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Classifier\n",
    "We will use sklearn's dummy classifier as a baseline for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_list = ['2013-06-30', '2013-05-31', '2013-04-30']\n",
    "end_list = ['2013-12-31', '2013-11-30','2013-10-31']\n",
    "thresholds = [.01,.02,.05,.1,.2,.3,.5]\n",
    "dummy = []\n",
    "\n",
    "for i, date in enumerate(start_list):\n",
    "    end_date = end_list[i]\n",
    "    for t in thresholds:\n",
    "        model_name = 'dummy_' + date + '_' + str(t)\n",
    "        d_test, d_predict, d_score = pipeline_revised.dummy_baseline(df, 'time_tf', threshold = t,\n",
    "                                                                               predictors=['total_price_including_optional_support', 'students_reached','poverty_level_high poverty', 'poverty_level_highest poverty',\n",
    "       'poverty_level_low poverty', 'poverty_level_moderate poverty','resource_type_Books', 'resource_type_Other', 'resource_type_Supplies',\n",
    "       'resource_type_Technology', 'resource_type_Trips',\n",
    "       'resource_type_Visitors','school_metro_rural',\n",
    "       'school_metro_suburban', 'school_metro_urban','primary_focus_subject_Applied Sciences',\n",
    "       'primary_focus_subject_Character Education',\n",
    "       'primary_focus_subject_Civics & Government',\n",
    "       'primary_focus_subject_College & Career Prep',\n",
    "       'primary_focus_subject_Community Service', 'primary_focus_subject_ESL',\n",
    "       'primary_focus_subject_Early Development',\n",
    "       'primary_focus_subject_Economics',\n",
    "       'primary_focus_subject_Environmental Science',\n",
    "       'primary_focus_subject_Extracurricular',\n",
    "       'primary_focus_subject_Foreign Languages',\n",
    "       'primary_focus_subject_Gym & Fitness',\n",
    "       'primary_focus_subject_Health & Life Science',\n",
    "       'primary_focus_subject_Health & Wellness',\n",
    "       'primary_focus_subject_History & Geography',\n",
    "       'primary_focus_subject_Literacy',\n",
    "       'primary_focus_subject_Literature & Writing',\n",
    "       'primary_focus_subject_Mathematics', 'primary_focus_subject_Music',\n",
    "       'primary_focus_subject_Nutrition', 'primary_focus_subject_Other',\n",
    "       'primary_focus_subject_Parent Involvement',\n",
    "       'primary_focus_subject_Performing Arts',\n",
    "       'primary_focus_subject_Social Sciences',\n",
    "       'primary_focus_subject_Special Needs', 'primary_focus_subject_Sports',\n",
    "       'primary_focus_subject_Visual Arts'],temporal=True, start_col='date_posted',end_col='datefullyfunded',start_date=date,end_date=end_date)\n",
    "        dummy.append((model_name, d_test, d_predict, d_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the performance metrics for the dummy variables generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_metrics = pd.DataFrame(columns=['name','accuracy','precision','auc','recall','f1'])\n",
    "\n",
    "for i in range(len(dummy)):\n",
    "    name, test, predict, _ = dummy[i]\n",
    "    accuracy, precision, auc, recall, f1 = pipeline_revised.evaluate_model(test, predict)\n",
    "    dummy_metrics.loc[i,'name'] = name\n",
    "    dummy_metrics.loc[i,'accuracy'] = accuracy\n",
    "    dummy_metrics.loc[i,'precision'] = precision\n",
    "    dummy_metrics.loc[i,'auc'] = auc\n",
    "    dummy_metrics.loc[i,'recall'] = recall\n",
    "    dummy_metrics.loc[i,'f1'] = f1\n",
    "\n",
    "dummy_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see here, the performance of the dummy model on dimensions like AUC/accuracy does not achieve that of the \"highest performing\" models on those dimensions; however, it is not that far off. This will be described in the write-up in more depth."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
